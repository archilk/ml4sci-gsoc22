{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrtpILJrYTU4"
      },
      "outputs": [],
      "source": [
        "# NOTE:\n",
        "\n",
        "# Because there are 3 classes, the AUC is calculated as the average of the AUCs of the individual classes.\n",
        "\n",
        "# The given train set is divided in a 90:10 ratio into \"train\" and \"validation\" sets. This \"validation\" set is used for hyperparameter tuning.\n",
        "# The given val set is considered as the external train set.\n",
        "# Thus, there are 3 sets in total."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDb3PUZ5BuU_"
      },
      "outputs": [],
      "source": [
        "!unzip './data' -d './data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARsPvtpwCaNp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plot\n",
        "\n",
        "import os\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sODoODigqSJn"
      },
      "outputs": [],
      "source": [
        "# Set random seed\n",
        "SEED = 0\n",
        "keras.utils.set_random_seed(SEED)\n",
        "tf.config.experimental.enable_op_determinism()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnRkSbntGjWi"
      },
      "outputs": [],
      "source": [
        "BASEPATH = './data/dataset' # Location of data\n",
        "MODELPATH = './weights' # Location of weights\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 300\n",
        "LEARNING_RATE = 5e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuBQWR4nCk7L"
      },
      "outputs": [],
      "source": [
        "def load_data(datatype):\n",
        "  x, y = [], []\n",
        "  for index, category in enumerate(['no', 'sphere', 'vort']): # Labels: no=0, sphere=1, vort=2\n",
        "    category_path = os.path.join(BASEPATH, datatype, category)\n",
        "    for filename in os.listdir(category_path):\n",
        "      filedata = np.load(os.path.join(category_path, filename))\n",
        "      x.append(filedata)\n",
        "      label = np.stack([np.eye(3)[index] for _ in range(len(filedata))], axis=0) # make one-hot encoding\n",
        "      y.append(label)\n",
        "  x = np.expand_dims(np.concatenate(x, axis=0), axis=-1) # add channel axis\n",
        "  y = np.concatenate(y, axis=0)\n",
        "\n",
        "  # Save\n",
        "  np.save(os.path.join(BASEPATH, f'X_{datatype}.npy'), x)\n",
        "  np.save(os.path.join(BASEPATH, f'Y_{datatype}.npy'), y)\n",
        "\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNHLJdHrDDp7"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "X_TRAIN, Y_TRAIN = load_data('train')\n",
        "X_VAL, Y_VAL = load_data('val')\n",
        "\n",
        "# Standardize data\n",
        "X_TRAIN = ((X_TRAIN - np.mean(X_TRAIN, axis=(1, 2), keepdims=True))\n",
        "          / np.std(X_TRAIN, axis=(1, 2), keepdims=True))\n",
        "X_VAL = ((X_VAL - np.mean(X_VAL, axis=(1, 2), keepdims=True))\n",
        "          / np.std(X_VAL, axis=(1, 2), keepdims=True))\n",
        "\n",
        "# Shuffle data\n",
        "perm = np.random.permutation(len(X_TRAIN))\n",
        "X_TRAIN, Y_TRAIN = X_TRAIN[perm], Y_TRAIN[perm]\n",
        "perm = np.random.permutation(len(X_VAL))\n",
        "X_VAL, Y_VAL = X_VAL[perm], Y_VAL[perm]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh61he1UCpks"
      },
      "outputs": [],
      "source": [
        "def make_model(input_shape):\n",
        "  inputs = keras.Input(shape=input_shape)\n",
        "  x = keras.Sequential([\n",
        "                        layers.RandomFlip(),\n",
        "                        layers.RandomRotation(0.5)\n",
        "                        ])(inputs)\n",
        "  \n",
        "  x = layers.Conv2D(16, 5)(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Activation('relu')(x)\n",
        "\n",
        "  for _ in range(3):\n",
        "    x = layers.Conv2D(32, 3)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.MaxPool2D()(x)\n",
        "  \n",
        "  for _ in range(2):\n",
        "    residual = layers.Conv2D(64, 1)(x)\n",
        "    x = layers.Conv2D(64, 3, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "\n",
        "  x = layers.Conv2D(128, 3)(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Activation('relu')(x)\n",
        "\n",
        "  x = layers.Flatten()(x)\n",
        "\n",
        "  x = layers.Dense(256, activation='relu')(x)\n",
        "  x = layers.Dense(32, activation='relu')(x)\n",
        "  outputs = layers.Dense(3, activation='softmax')(x)\n",
        "\n",
        "  return keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2dEngfR1-eS"
      },
      "outputs": [],
      "source": [
        "class TrackBestPerformance(keras.callbacks.Callback):\n",
        "  \"\"\"\n",
        "  Callback to keep track of model weights which give best val_auc\n",
        "  After training completes, the model is assigned with the best model weights\n",
        "  \"\"\"\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.best_auc = 0\n",
        "    self.best_epoch = -1\n",
        "    self.best_weights = None\n",
        "  \n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    current_auc = logs['val_auc']\n",
        "    if current_auc > self.best_auc:\n",
        "      self.best_auc = current_auc\n",
        "      self.best_epoch = epoch\n",
        "      self.best_weights = self.model.get_weights()\n",
        "  \n",
        "  def on_train_end(self, logs=None):\n",
        "    self.model.set_weights(self.best_weights)\n",
        "    print(f'Best validation AUC is {self.best_auc} on epoch #{self.best_epoch + 1}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykEHiwaxC72y",
        "outputId": "f6f8b30f-978d-4801-93fa-37d030365909"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "  6/106 [>.............................] - ETA: 33s - loss: 4.3963 - auc: 0.4969WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1184s vs `on_train_batch_end` time: 0.1782s). Check your callbacks.\n",
            "106/106 [==============================] - 53s 381ms/step - loss: 1.4276 - auc: 0.5076 - val_loss: 1.1014 - val_auc: 0.5092\n",
            "Epoch 2/300\n",
            "106/106 [==============================] - 38s 361ms/step - loss: 1.1383 - auc: 0.5133 - val_loss: 1.1180 - val_auc: 0.5153\n",
            "Epoch 3/300\n",
            "106/106 [==============================] - 38s 355ms/step - loss: 1.1321 - auc: 0.5153 - val_loss: 1.1011 - val_auc: 0.5321\n",
            "Epoch 4/300\n",
            "106/106 [==============================] - 38s 359ms/step - loss: 1.1145 - auc: 0.5256 - val_loss: 1.0968 - val_auc: 0.5531\n",
            "Epoch 5/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 1.0764 - auc: 0.5854 - val_loss: 1.2081 - val_auc: 0.6512\n",
            "Epoch 6/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.9932 - auc: 0.6764 - val_loss: 0.9998 - val_auc: 0.6853\n",
            "Epoch 7/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.9176 - auc: 0.7321 - val_loss: 1.2216 - val_auc: 0.7199\n",
            "Epoch 8/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.8604 - auc: 0.7677 - val_loss: 2.0228 - val_auc: 0.7270\n",
            "Epoch 9/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.8210 - auc: 0.7907 - val_loss: 1.1359 - val_auc: 0.7573\n",
            "Epoch 10/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.7932 - auc: 0.8060 - val_loss: 1.3273 - val_auc: 0.7947\n",
            "Epoch 11/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.7386 - auc: 0.8336 - val_loss: 1.2509 - val_auc: 0.7805\n",
            "Epoch 12/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.7164 - auc: 0.8441 - val_loss: 0.7626 - val_auc: 0.8481\n",
            "Epoch 13/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.6921 - auc: 0.8560 - val_loss: 0.8981 - val_auc: 0.8425\n",
            "Epoch 14/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.6404 - auc: 0.8775 - val_loss: 0.7409 - val_auc: 0.8777\n",
            "Epoch 15/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.5942 - auc: 0.8946 - val_loss: 0.6780 - val_auc: 0.8709\n",
            "Epoch 16/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.5668 - auc: 0.9048 - val_loss: 0.5753 - val_auc: 0.9127\n",
            "Epoch 17/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.5085 - auc: 0.9231 - val_loss: 1.0089 - val_auc: 0.8859\n",
            "Epoch 18/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.4764 - auc: 0.9327 - val_loss: 1.5670 - val_auc: 0.8628\n",
            "Epoch 19/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.4535 - auc: 0.9390 - val_loss: 0.7284 - val_auc: 0.9133\n",
            "Epoch 20/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.4298 - auc: 0.9453 - val_loss: 0.7815 - val_auc: 0.9225\n",
            "Epoch 21/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.4093 - auc: 0.9502 - val_loss: 0.4722 - val_auc: 0.9471\n",
            "Epoch 22/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.3929 - auc: 0.9543 - val_loss: 0.4638 - val_auc: 0.9487\n",
            "Epoch 23/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.3763 - auc: 0.9578 - val_loss: 0.9160 - val_auc: 0.9250\n",
            "Epoch 24/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.3562 - auc: 0.9619 - val_loss: 0.7976 - val_auc: 0.9267\n",
            "Epoch 25/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.3642 - auc: 0.9603 - val_loss: 0.5652 - val_auc: 0.9442\n",
            "Epoch 26/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.3405 - auc: 0.9650 - val_loss: 0.5244 - val_auc: 0.9548\n",
            "Epoch 27/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.3314 - auc: 0.9668 - val_loss: 0.4796 - val_auc: 0.9592\n",
            "Epoch 28/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.3211 - auc: 0.9686 - val_loss: 0.8970 - val_auc: 0.9243\n",
            "Epoch 29/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.3152 - auc: 0.9697 - val_loss: 0.3190 - val_auc: 0.9710\n",
            "Epoch 30/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.3148 - auc: 0.9697 - val_loss: 0.7219 - val_auc: 0.9418\n",
            "Epoch 31/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.3099 - auc: 0.9707 - val_loss: 0.3498 - val_auc: 0.9704\n",
            "Epoch 32/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2979 - auc: 0.9727 - val_loss: 0.3638 - val_auc: 0.9693\n",
            "Epoch 33/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.3111 - auc: 0.9705 - val_loss: 0.3748 - val_auc: 0.9662\n",
            "Epoch 34/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2940 - auc: 0.9736 - val_loss: 0.4528 - val_auc: 0.9506\n",
            "Epoch 35/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.2870 - auc: 0.9747 - val_loss: 0.3213 - val_auc: 0.9690\n",
            "Epoch 36/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2883 - auc: 0.9744 - val_loss: 0.3889 - val_auc: 0.9687\n",
            "Epoch 37/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2816 - auc: 0.9754 - val_loss: 0.3150 - val_auc: 0.9745\n",
            "Epoch 38/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.2826 - auc: 0.9754 - val_loss: 0.3312 - val_auc: 0.9675\n",
            "Epoch 39/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.2765 - auc: 0.9764 - val_loss: 0.3193 - val_auc: 0.9721\n",
            "Epoch 40/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2725 - auc: 0.9768 - val_loss: 0.3280 - val_auc: 0.9676\n",
            "Epoch 41/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2618 - auc: 0.9786 - val_loss: 0.6451 - val_auc: 0.9552\n",
            "Epoch 42/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.2619 - auc: 0.9786 - val_loss: 0.3086 - val_auc: 0.9726\n",
            "Epoch 43/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2578 - auc: 0.9791 - val_loss: 1.2049 - val_auc: 0.9316\n",
            "Epoch 44/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.2552 - auc: 0.9795 - val_loss: 0.2904 - val_auc: 0.9744\n",
            "Epoch 45/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2512 - auc: 0.9801 - val_loss: 0.2784 - val_auc: 0.9783\n",
            "Epoch 46/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2507 - auc: 0.9803 - val_loss: 0.5437 - val_auc: 0.9655\n",
            "Epoch 47/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.2419 - auc: 0.9815 - val_loss: 0.3312 - val_auc: 0.9699\n",
            "Epoch 48/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2425 - auc: 0.9813 - val_loss: 0.3196 - val_auc: 0.9764\n",
            "Epoch 49/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2501 - auc: 0.9802 - val_loss: 0.7763 - val_auc: 0.9564\n",
            "Epoch 50/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2396 - auc: 0.9818 - val_loss: 0.3802 - val_auc: 0.9678\n",
            "Epoch 51/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2346 - auc: 0.9823 - val_loss: 0.3323 - val_auc: 0.9690\n",
            "Epoch 52/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2416 - auc: 0.9814 - val_loss: 0.3614 - val_auc: 0.9752\n",
            "Epoch 53/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2343 - auc: 0.9823 - val_loss: 0.5623 - val_auc: 0.9563\n",
            "Epoch 54/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2357 - auc: 0.9823 - val_loss: 0.3138 - val_auc: 0.9772\n",
            "Epoch 55/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.2342 - auc: 0.9827 - val_loss: 0.4496 - val_auc: 0.9683\n",
            "Epoch 56/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.2340 - auc: 0.9823 - val_loss: 0.2499 - val_auc: 0.9816\n",
            "Epoch 57/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2197 - auc: 0.9846 - val_loss: 0.4886 - val_auc: 0.9605\n",
            "Epoch 58/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2251 - auc: 0.9836 - val_loss: 0.3174 - val_auc: 0.9761\n",
            "Epoch 59/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2244 - auc: 0.9836 - val_loss: 0.3718 - val_auc: 0.9755\n",
            "Epoch 60/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.2229 - auc: 0.9839 - val_loss: 0.2847 - val_auc: 0.9767\n",
            "Epoch 61/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2184 - auc: 0.9847 - val_loss: 0.2464 - val_auc: 0.9837\n",
            "Epoch 62/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2129 - auc: 0.9850 - val_loss: 0.2952 - val_auc: 0.9801\n",
            "Epoch 63/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2166 - auc: 0.9847 - val_loss: 0.2414 - val_auc: 0.9843\n",
            "Epoch 64/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2113 - auc: 0.9853 - val_loss: 0.2770 - val_auc: 0.9787\n",
            "Epoch 65/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.2152 - auc: 0.9850 - val_loss: 0.2592 - val_auc: 0.9809\n",
            "Epoch 66/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.2109 - auc: 0.9854 - val_loss: 0.2147 - val_auc: 0.9854\n",
            "Epoch 67/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2087 - auc: 0.9858 - val_loss: 0.4396 - val_auc: 0.9728\n",
            "Epoch 68/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2074 - auc: 0.9860 - val_loss: 0.3186 - val_auc: 0.9848\n",
            "Epoch 69/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.2047 - auc: 0.9862 - val_loss: 0.2928 - val_auc: 0.9800\n",
            "Epoch 70/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.2057 - auc: 0.9861 - val_loss: 0.2424 - val_auc: 0.9828\n",
            "Epoch 71/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1999 - auc: 0.9866 - val_loss: 0.2442 - val_auc: 0.9814\n",
            "Epoch 72/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.2026 - auc: 0.9867 - val_loss: 0.3023 - val_auc: 0.9782\n",
            "Epoch 73/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.2025 - auc: 0.9865 - val_loss: 0.2808 - val_auc: 0.9829\n",
            "Epoch 74/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1991 - auc: 0.9869 - val_loss: 0.2392 - val_auc: 0.9813\n",
            "Epoch 75/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1951 - auc: 0.9873 - val_loss: 0.7977 - val_auc: 0.9546\n",
            "Epoch 76/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1984 - auc: 0.9868 - val_loss: 0.5384 - val_auc: 0.9625\n",
            "Epoch 77/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1893 - auc: 0.9881 - val_loss: 0.1998 - val_auc: 0.9870\n",
            "Epoch 78/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1940 - auc: 0.9877 - val_loss: 0.2379 - val_auc: 0.9849\n",
            "Epoch 79/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1941 - auc: 0.9875 - val_loss: 0.4167 - val_auc: 0.9722\n",
            "Epoch 80/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1992 - auc: 0.9868 - val_loss: 0.2274 - val_auc: 0.9847\n",
            "Epoch 81/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1925 - auc: 0.9875 - val_loss: 0.2270 - val_auc: 0.9859\n",
            "Epoch 82/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1861 - auc: 0.9883 - val_loss: 0.2127 - val_auc: 0.9873\n",
            "Epoch 83/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1932 - auc: 0.9873 - val_loss: 0.6054 - val_auc: 0.9590\n",
            "Epoch 84/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1850 - auc: 0.9885 - val_loss: 0.2259 - val_auc: 0.9849\n",
            "Epoch 85/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1811 - auc: 0.9890 - val_loss: 0.2418 - val_auc: 0.9823\n",
            "Epoch 86/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1856 - auc: 0.9884 - val_loss: 0.2773 - val_auc: 0.9796\n",
            "Epoch 87/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1844 - auc: 0.9885 - val_loss: 0.1885 - val_auc: 0.9875\n",
            "Epoch 88/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.1792 - auc: 0.9892 - val_loss: 0.2698 - val_auc: 0.9779\n",
            "Epoch 89/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1815 - auc: 0.9890 - val_loss: 0.3790 - val_auc: 0.9721\n",
            "Epoch 90/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1836 - auc: 0.9887 - val_loss: 0.3977 - val_auc: 0.9777\n",
            "Epoch 91/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1824 - auc: 0.9887 - val_loss: 0.2787 - val_auc: 0.9819\n",
            "Epoch 92/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1793 - auc: 0.9890 - val_loss: 0.4759 - val_auc: 0.9712\n",
            "Epoch 93/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1825 - auc: 0.9889 - val_loss: 0.2620 - val_auc: 0.9816\n",
            "Epoch 94/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1799 - auc: 0.9888 - val_loss: 0.1938 - val_auc: 0.9871\n",
            "Epoch 95/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.1795 - auc: 0.9892 - val_loss: 0.2201 - val_auc: 0.9835\n",
            "Epoch 96/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.1757 - auc: 0.9895 - val_loss: 0.2145 - val_auc: 0.9866\n",
            "Epoch 97/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1765 - auc: 0.9894 - val_loss: 0.4347 - val_auc: 0.9808\n",
            "Epoch 98/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1692 - auc: 0.9904 - val_loss: 0.2541 - val_auc: 0.9854\n",
            "Epoch 99/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1765 - auc: 0.9893 - val_loss: 0.2588 - val_auc: 0.9860\n",
            "Epoch 100/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.1721 - auc: 0.9898 - val_loss: 0.6251 - val_auc: 0.9574\n",
            "Epoch 101/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1757 - auc: 0.9896 - val_loss: 0.1932 - val_auc: 0.9870\n",
            "Epoch 102/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1693 - auc: 0.9901 - val_loss: 0.2449 - val_auc: 0.9853\n",
            "Epoch 103/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1734 - auc: 0.9897 - val_loss: 0.2274 - val_auc: 0.9837\n",
            "Epoch 104/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1692 - auc: 0.9903 - val_loss: 0.1725 - val_auc: 0.9897\n",
            "Epoch 105/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.1687 - auc: 0.9903 - val_loss: 0.2202 - val_auc: 0.9841\n",
            "Epoch 106/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.1673 - auc: 0.9904 - val_loss: 0.2536 - val_auc: 0.9811\n",
            "Epoch 107/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1662 - auc: 0.9906 - val_loss: 0.3119 - val_auc: 0.9778\n",
            "Epoch 108/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1634 - auc: 0.9907 - val_loss: 0.4829 - val_auc: 0.9694\n",
            "Epoch 109/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1665 - auc: 0.9904 - val_loss: 0.7507 - val_auc: 0.9491\n",
            "Epoch 110/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1604 - auc: 0.9912 - val_loss: 0.4106 - val_auc: 0.9758\n",
            "Epoch 111/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1647 - auc: 0.9909 - val_loss: 0.1873 - val_auc: 0.9891\n",
            "Epoch 112/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1617 - auc: 0.9911 - val_loss: 0.2038 - val_auc: 0.9879\n",
            "Epoch 113/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.1612 - auc: 0.9909 - val_loss: 0.2847 - val_auc: 0.9849\n",
            "Epoch 114/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1634 - auc: 0.9909 - val_loss: 0.2168 - val_auc: 0.9846\n",
            "Epoch 115/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1609 - auc: 0.9910 - val_loss: 0.1983 - val_auc: 0.9878\n",
            "Epoch 116/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1627 - auc: 0.9908 - val_loss: 0.1964 - val_auc: 0.9873\n",
            "Epoch 117/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1573 - auc: 0.9914 - val_loss: 0.3033 - val_auc: 0.9833\n",
            "Epoch 118/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.1598 - auc: 0.9911 - val_loss: 0.5514 - val_auc: 0.9708\n",
            "Epoch 119/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1591 - auc: 0.9911 - val_loss: 0.2499 - val_auc: 0.9813\n",
            "Epoch 120/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1573 - auc: 0.9915 - val_loss: 0.3471 - val_auc: 0.9773\n",
            "Epoch 121/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.1561 - auc: 0.9914 - val_loss: 0.2528 - val_auc: 0.9855\n",
            "Epoch 122/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1527 - auc: 0.9917 - val_loss: 0.7536 - val_auc: 0.9548\n",
            "Epoch 123/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1582 - auc: 0.9914 - val_loss: 0.2037 - val_auc: 0.9877\n",
            "Epoch 124/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1561 - auc: 0.9916 - val_loss: 0.2695 - val_auc: 0.9826\n",
            "Epoch 125/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.1501 - auc: 0.9920 - val_loss: 0.2108 - val_auc: 0.9867\n",
            "Epoch 126/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1559 - auc: 0.9917 - val_loss: 0.2177 - val_auc: 0.9864\n",
            "Epoch 127/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1544 - auc: 0.9916 - val_loss: 0.3995 - val_auc: 0.9741\n",
            "Epoch 128/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1540 - auc: 0.9918 - val_loss: 0.4122 - val_auc: 0.9755\n",
            "Epoch 129/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1534 - auc: 0.9919 - val_loss: 0.1897 - val_auc: 0.9876\n",
            "Epoch 130/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1479 - auc: 0.9922 - val_loss: 0.7430 - val_auc: 0.9642\n",
            "Epoch 131/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1489 - auc: 0.9923 - val_loss: 0.2187 - val_auc: 0.9889\n",
            "Epoch 132/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1492 - auc: 0.9923 - val_loss: 0.2030 - val_auc: 0.9867\n",
            "Epoch 133/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1534 - auc: 0.9916 - val_loss: 0.2735 - val_auc: 0.9860\n",
            "Epoch 134/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1511 - auc: 0.9920 - val_loss: 0.2017 - val_auc: 0.9866\n",
            "Epoch 135/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1465 - auc: 0.9925 - val_loss: 0.4288 - val_auc: 0.9773\n",
            "Epoch 136/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1480 - auc: 0.9922 - val_loss: 0.2200 - val_auc: 0.9850\n",
            "Epoch 137/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1441 - auc: 0.9925 - val_loss: 0.3151 - val_auc: 0.9791\n",
            "Epoch 138/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1420 - auc: 0.9927 - val_loss: 0.1808 - val_auc: 0.9894\n",
            "Epoch 139/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1431 - auc: 0.9928 - val_loss: 0.1890 - val_auc: 0.9887\n",
            "Epoch 140/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1483 - auc: 0.9921 - val_loss: 0.1898 - val_auc: 0.9881\n",
            "Epoch 141/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1515 - auc: 0.9920 - val_loss: 0.2855 - val_auc: 0.9837\n",
            "Epoch 142/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1447 - auc: 0.9927 - val_loss: 0.2079 - val_auc: 0.9885\n",
            "Epoch 143/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1432 - auc: 0.9927 - val_loss: 0.3227 - val_auc: 0.9819\n",
            "Epoch 144/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1403 - auc: 0.9929 - val_loss: 0.2485 - val_auc: 0.9852\n",
            "Epoch 145/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1447 - auc: 0.9925 - val_loss: 0.2307 - val_auc: 0.9851\n",
            "Epoch 146/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1432 - auc: 0.9928 - val_loss: 0.3453 - val_auc: 0.9821\n",
            "Epoch 147/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1383 - auc: 0.9931 - val_loss: 0.2413 - val_auc: 0.9877\n",
            "Epoch 148/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1453 - auc: 0.9926 - val_loss: 0.1930 - val_auc: 0.9889\n",
            "Epoch 149/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1391 - auc: 0.9932 - val_loss: 0.2619 - val_auc: 0.9849\n",
            "Epoch 150/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1400 - auc: 0.9930 - val_loss: 0.4568 - val_auc: 0.9742\n",
            "Epoch 151/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1359 - auc: 0.9934 - val_loss: 0.1842 - val_auc: 0.9883\n",
            "Epoch 152/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1366 - auc: 0.9932 - val_loss: 0.2945 - val_auc: 0.9834\n",
            "Epoch 153/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1423 - auc: 0.9929 - val_loss: 0.2654 - val_auc: 0.9815\n",
            "Epoch 154/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1356 - auc: 0.9934 - val_loss: 0.1671 - val_auc: 0.9897\n",
            "Epoch 155/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1364 - auc: 0.9932 - val_loss: 0.1758 - val_auc: 0.9893\n",
            "Epoch 156/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1299 - auc: 0.9940 - val_loss: 0.1901 - val_auc: 0.9876\n",
            "Epoch 157/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1368 - auc: 0.9932 - val_loss: 0.2020 - val_auc: 0.9855\n",
            "Epoch 158/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1321 - auc: 0.9938 - val_loss: 0.2943 - val_auc: 0.9861\n",
            "Epoch 159/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1373 - auc: 0.9932 - val_loss: 0.2113 - val_auc: 0.9851\n",
            "Epoch 160/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1314 - auc: 0.9938 - val_loss: 0.2469 - val_auc: 0.9850\n",
            "Epoch 161/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1364 - auc: 0.9935 - val_loss: 0.3557 - val_auc: 0.9840\n",
            "Epoch 162/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1321 - auc: 0.9937 - val_loss: 0.2170 - val_auc: 0.9887\n",
            "Epoch 163/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1322 - auc: 0.9937 - val_loss: 0.2161 - val_auc: 0.9873\n",
            "Epoch 164/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1306 - auc: 0.9938 - val_loss: 0.2056 - val_auc: 0.9866\n",
            "Epoch 165/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1331 - auc: 0.9934 - val_loss: 0.2210 - val_auc: 0.9881\n",
            "Epoch 166/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1330 - auc: 0.9936 - val_loss: 0.1667 - val_auc: 0.9900\n",
            "Epoch 167/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1313 - auc: 0.9938 - val_loss: 0.1738 - val_auc: 0.9899\n",
            "Epoch 168/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1308 - auc: 0.9939 - val_loss: 0.1776 - val_auc: 0.9894\n",
            "Epoch 169/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1315 - auc: 0.9938 - val_loss: 0.1928 - val_auc: 0.9873\n",
            "Epoch 170/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1385 - auc: 0.9932 - val_loss: 0.2268 - val_auc: 0.9849\n",
            "Epoch 171/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1296 - auc: 0.9938 - val_loss: 0.1722 - val_auc: 0.9896\n",
            "Epoch 172/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1267 - auc: 0.9942 - val_loss: 1.1287 - val_auc: 0.9250\n",
            "Epoch 173/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1289 - auc: 0.9939 - val_loss: 0.2420 - val_auc: 0.9837\n",
            "Epoch 174/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1263 - auc: 0.9942 - val_loss: 0.4072 - val_auc: 0.9821\n",
            "Epoch 175/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1300 - auc: 0.9939 - val_loss: 0.3334 - val_auc: 0.9761\n",
            "Epoch 176/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1287 - auc: 0.9941 - val_loss: 0.3625 - val_auc: 0.9864\n",
            "Epoch 177/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1280 - auc: 0.9940 - val_loss: 0.1454 - val_auc: 0.9919\n",
            "Epoch 178/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1247 - auc: 0.9944 - val_loss: 0.3381 - val_auc: 0.9819\n",
            "Epoch 179/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1248 - auc: 0.9944 - val_loss: 0.2142 - val_auc: 0.9888\n",
            "Epoch 180/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1277 - auc: 0.9941 - val_loss: 0.2502 - val_auc: 0.9846\n",
            "Epoch 181/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1259 - auc: 0.9942 - val_loss: 0.6455 - val_auc: 0.9578\n",
            "Epoch 182/300\n",
            "106/106 [==============================] - 38s 359ms/step - loss: 0.1260 - auc: 0.9942 - val_loss: 0.2301 - val_auc: 0.9866\n",
            "Epoch 183/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1217 - auc: 0.9944 - val_loss: 0.2466 - val_auc: 0.9824\n",
            "Epoch 184/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1242 - auc: 0.9944 - val_loss: 0.3824 - val_auc: 0.9814\n",
            "Epoch 185/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1224 - auc: 0.9947 - val_loss: 0.1830 - val_auc: 0.9890\n",
            "Epoch 186/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1269 - auc: 0.9941 - val_loss: 0.1617 - val_auc: 0.9909\n",
            "Epoch 187/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1201 - auc: 0.9946 - val_loss: 0.1527 - val_auc: 0.9909\n",
            "Epoch 188/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1249 - auc: 0.9944 - val_loss: 0.1619 - val_auc: 0.9912\n",
            "Epoch 189/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1255 - auc: 0.9943 - val_loss: 0.1928 - val_auc: 0.9889\n",
            "Epoch 190/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1218 - auc: 0.9945 - val_loss: 0.4967 - val_auc: 0.9703\n",
            "Epoch 191/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1235 - auc: 0.9944 - val_loss: 0.3001 - val_auc: 0.9833\n",
            "Epoch 192/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1217 - auc: 0.9947 - val_loss: 0.1624 - val_auc: 0.9901\n",
            "Epoch 193/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1192 - auc: 0.9947 - val_loss: 0.1733 - val_auc: 0.9887\n",
            "Epoch 194/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1195 - auc: 0.9948 - val_loss: 0.3281 - val_auc: 0.9774\n",
            "Epoch 195/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1248 - auc: 0.9944 - val_loss: 0.1969 - val_auc: 0.9887\n",
            "Epoch 196/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1214 - auc: 0.9947 - val_loss: 0.3640 - val_auc: 0.9755\n",
            "Epoch 197/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1169 - auc: 0.9950 - val_loss: 0.1778 - val_auc: 0.9897\n",
            "Epoch 198/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1225 - auc: 0.9946 - val_loss: 0.1682 - val_auc: 0.9907\n",
            "Epoch 199/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1169 - auc: 0.9950 - val_loss: 0.1757 - val_auc: 0.9892\n",
            "Epoch 200/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1177 - auc: 0.9948 - val_loss: 0.1730 - val_auc: 0.9898\n",
            "Epoch 201/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1164 - auc: 0.9951 - val_loss: 0.1792 - val_auc: 0.9887\n",
            "Epoch 202/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1174 - auc: 0.9949 - val_loss: 1.3785 - val_auc: 0.9106\n",
            "Epoch 203/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1146 - auc: 0.9952 - val_loss: 0.1654 - val_auc: 0.9900\n",
            "Epoch 204/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1176 - auc: 0.9947 - val_loss: 0.1761 - val_auc: 0.9905\n",
            "Epoch 205/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1163 - auc: 0.9951 - val_loss: 0.1979 - val_auc: 0.9880\n",
            "Epoch 206/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1162 - auc: 0.9950 - val_loss: 0.2764 - val_auc: 0.9831\n",
            "Epoch 207/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1144 - auc: 0.9951 - val_loss: 0.1678 - val_auc: 0.9918\n",
            "Epoch 208/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1140 - auc: 0.9953 - val_loss: 0.2174 - val_auc: 0.9874\n",
            "Epoch 209/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1158 - auc: 0.9949 - val_loss: 0.2054 - val_auc: 0.9901\n",
            "Epoch 210/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1121 - auc: 0.9952 - val_loss: 0.2036 - val_auc: 0.9874\n",
            "Epoch 211/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1133 - auc: 0.9953 - val_loss: 0.3721 - val_auc: 0.9812\n",
            "Epoch 212/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1198 - auc: 0.9947 - val_loss: 0.1677 - val_auc: 0.9907\n",
            "Epoch 213/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1182 - auc: 0.9948 - val_loss: 0.2235 - val_auc: 0.9858\n",
            "Epoch 214/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1138 - auc: 0.9952 - val_loss: 0.3353 - val_auc: 0.9775\n",
            "Epoch 215/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1150 - auc: 0.9950 - val_loss: 0.4221 - val_auc: 0.9796\n",
            "Epoch 216/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1100 - auc: 0.9954 - val_loss: 0.2060 - val_auc: 0.9893\n",
            "Epoch 217/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1089 - auc: 0.9956 - val_loss: 0.7499 - val_auc: 0.9581\n",
            "Epoch 218/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1119 - auc: 0.9952 - val_loss: 0.1650 - val_auc: 0.9908\n",
            "Epoch 219/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1139 - auc: 0.9952 - val_loss: 0.2053 - val_auc: 0.9881\n",
            "Epoch 220/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1136 - auc: 0.9951 - val_loss: 0.2187 - val_auc: 0.9872\n",
            "Epoch 221/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1134 - auc: 0.9952 - val_loss: 0.3265 - val_auc: 0.9869\n",
            "Epoch 222/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1076 - auc: 0.9957 - val_loss: 0.1445 - val_auc: 0.9922\n",
            "Epoch 223/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1140 - auc: 0.9953 - val_loss: 0.1941 - val_auc: 0.9884\n",
            "Epoch 224/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1150 - auc: 0.9950 - val_loss: 0.1844 - val_auc: 0.9882\n",
            "Epoch 225/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1090 - auc: 0.9956 - val_loss: 0.1581 - val_auc: 0.9907\n",
            "Epoch 226/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1134 - auc: 0.9952 - val_loss: 0.1698 - val_auc: 0.9906\n",
            "Epoch 227/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1082 - auc: 0.9956 - val_loss: 0.2260 - val_auc: 0.9860\n",
            "Epoch 228/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1086 - auc: 0.9957 - val_loss: 0.5903 - val_auc: 0.9756\n",
            "Epoch 229/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1089 - auc: 0.9954 - val_loss: 0.1576 - val_auc: 0.9912\n",
            "Epoch 230/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1148 - auc: 0.9953 - val_loss: 0.1673 - val_auc: 0.9892\n",
            "Epoch 231/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1080 - auc: 0.9956 - val_loss: 0.1664 - val_auc: 0.9903\n",
            "Epoch 232/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1056 - auc: 0.9959 - val_loss: 0.1622 - val_auc: 0.9900\n",
            "Epoch 233/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1087 - auc: 0.9954 - val_loss: 0.1726 - val_auc: 0.9897\n",
            "Epoch 234/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.1064 - auc: 0.9957 - val_loss: 0.1689 - val_auc: 0.9912\n",
            "Epoch 235/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1080 - auc: 0.9953 - val_loss: 0.1528 - val_auc: 0.9911\n",
            "Epoch 236/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1057 - auc: 0.9959 - val_loss: 0.1613 - val_auc: 0.9904\n",
            "Epoch 237/300\n",
            "106/106 [==============================] - 38s 359ms/step - loss: 0.1123 - auc: 0.9952 - val_loss: 0.2442 - val_auc: 0.9859\n",
            "Epoch 238/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1115 - auc: 0.9954 - val_loss: 0.2928 - val_auc: 0.9834\n",
            "Epoch 239/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1039 - auc: 0.9959 - val_loss: 0.1442 - val_auc: 0.9924\n",
            "Epoch 240/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1064 - auc: 0.9957 - val_loss: 0.1883 - val_auc: 0.9893\n",
            "Epoch 241/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1085 - auc: 0.9955 - val_loss: 0.3458 - val_auc: 0.9808\n",
            "Epoch 242/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1079 - auc: 0.9956 - val_loss: 0.2672 - val_auc: 0.9842\n",
            "Epoch 243/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1018 - auc: 0.9960 - val_loss: 0.8093 - val_auc: 0.9460\n",
            "Epoch 244/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1075 - auc: 0.9954 - val_loss: 0.1768 - val_auc: 0.9887\n",
            "Epoch 245/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1018 - auc: 0.9961 - val_loss: 0.1654 - val_auc: 0.9889\n",
            "Epoch 246/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1043 - auc: 0.9957 - val_loss: 0.1640 - val_auc: 0.9901\n",
            "Epoch 247/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1063 - auc: 0.9957 - val_loss: 0.3056 - val_auc: 0.9858\n",
            "Epoch 248/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1045 - auc: 0.9958 - val_loss: 0.1618 - val_auc: 0.9907\n",
            "Epoch 249/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1038 - auc: 0.9960 - val_loss: 0.2698 - val_auc: 0.9853\n",
            "Epoch 250/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1020 - auc: 0.9960 - val_loss: 0.3775 - val_auc: 0.9805\n",
            "Epoch 251/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1037 - auc: 0.9960 - val_loss: 0.8898 - val_auc: 0.9470\n",
            "Epoch 252/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1050 - auc: 0.9959 - val_loss: 0.2021 - val_auc: 0.9886\n",
            "Epoch 253/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1047 - auc: 0.9959 - val_loss: 0.5482 - val_auc: 0.9744\n",
            "Epoch 254/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.1010 - auc: 0.9960 - val_loss: 0.4355 - val_auc: 0.9732\n",
            "Epoch 255/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.0976 - auc: 0.9962 - val_loss: 0.1966 - val_auc: 0.9886\n",
            "Epoch 256/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1010 - auc: 0.9959 - val_loss: 0.1775 - val_auc: 0.9896\n",
            "Epoch 257/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.1014 - auc: 0.9961 - val_loss: 0.7448 - val_auc: 0.9503\n",
            "Epoch 258/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1022 - auc: 0.9960 - val_loss: 0.1857 - val_auc: 0.9882\n",
            "Epoch 259/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1037 - auc: 0.9959 - val_loss: 0.1987 - val_auc: 0.9876\n",
            "Epoch 260/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.0991 - auc: 0.9964 - val_loss: 0.2745 - val_auc: 0.9827\n",
            "Epoch 261/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1012 - auc: 0.9962 - val_loss: 0.4874 - val_auc: 0.9682\n",
            "Epoch 262/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0987 - auc: 0.9962 - val_loss: 0.2681 - val_auc: 0.9825\n",
            "Epoch 263/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0963 - auc: 0.9964 - val_loss: 0.6712 - val_auc: 0.9658\n",
            "Epoch 264/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0967 - auc: 0.9962 - val_loss: 0.2418 - val_auc: 0.9890\n",
            "Epoch 265/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0956 - auc: 0.9963 - val_loss: 0.1956 - val_auc: 0.9881\n",
            "Epoch 266/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0984 - auc: 0.9963 - val_loss: 0.2145 - val_auc: 0.9899\n",
            "Epoch 267/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0967 - auc: 0.9963 - val_loss: 0.4680 - val_auc: 0.9732\n",
            "Epoch 268/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0955 - auc: 0.9964 - val_loss: 0.2220 - val_auc: 0.9849\n",
            "Epoch 269/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.0992 - auc: 0.9961 - val_loss: 0.2404 - val_auc: 0.9856\n",
            "Epoch 270/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0977 - auc: 0.9962 - val_loss: 0.4911 - val_auc: 0.9711\n",
            "Epoch 271/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0990 - auc: 0.9962 - val_loss: 0.1821 - val_auc: 0.9884\n",
            "Epoch 272/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.1005 - auc: 0.9961 - val_loss: 0.3034 - val_auc: 0.9837\n",
            "Epoch 273/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.0957 - auc: 0.9965 - val_loss: 0.4999 - val_auc: 0.9757\n",
            "Epoch 274/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0995 - auc: 0.9962 - val_loss: 0.2283 - val_auc: 0.9893\n",
            "Epoch 275/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.0944 - auc: 0.9965 - val_loss: 0.1756 - val_auc: 0.9892\n",
            "Epoch 276/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0942 - auc: 0.9965 - val_loss: 0.2635 - val_auc: 0.9865\n",
            "Epoch 277/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.0947 - auc: 0.9964 - val_loss: 0.1710 - val_auc: 0.9892\n",
            "Epoch 278/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0964 - auc: 0.9965 - val_loss: 0.2355 - val_auc: 0.9881\n",
            "Epoch 279/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.0945 - auc: 0.9965 - val_loss: 0.1924 - val_auc: 0.9905\n",
            "Epoch 280/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0980 - auc: 0.9964 - val_loss: 0.1769 - val_auc: 0.9876\n",
            "Epoch 281/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0949 - auc: 0.9966 - val_loss: 0.1935 - val_auc: 0.9877\n",
            "Epoch 282/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.0972 - auc: 0.9962 - val_loss: 0.2341 - val_auc: 0.9876\n",
            "Epoch 283/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0972 - auc: 0.9963 - val_loss: 0.3872 - val_auc: 0.9796\n",
            "Epoch 284/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0962 - auc: 0.9965 - val_loss: 0.2938 - val_auc: 0.9851\n",
            "Epoch 285/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.0965 - auc: 0.9963 - val_loss: 0.1575 - val_auc: 0.9910\n",
            "Epoch 286/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0943 - auc: 0.9965 - val_loss: 0.1756 - val_auc: 0.9910\n",
            "Epoch 287/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0949 - auc: 0.9964 - val_loss: 0.2494 - val_auc: 0.9851\n",
            "Epoch 288/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0985 - auc: 0.9963 - val_loss: 0.1636 - val_auc: 0.9898\n",
            "Epoch 289/300\n",
            "106/106 [==============================] - 38s 358ms/step - loss: 0.0893 - auc: 0.9969 - val_loss: 0.1862 - val_auc: 0.9880\n",
            "Epoch 290/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.0938 - auc: 0.9966 - val_loss: 0.1790 - val_auc: 0.9888\n",
            "Epoch 291/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0897 - auc: 0.9968 - val_loss: 0.2140 - val_auc: 0.9877\n",
            "Epoch 292/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0972 - auc: 0.9964 - val_loss: 0.3189 - val_auc: 0.9821\n",
            "Epoch 293/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0886 - auc: 0.9970 - val_loss: 0.5609 - val_auc: 0.9704\n",
            "Epoch 294/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0900 - auc: 0.9968 - val_loss: 0.2179 - val_auc: 0.9874\n",
            "Epoch 295/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.0941 - auc: 0.9965 - val_loss: 0.3219 - val_auc: 0.9813\n",
            "Epoch 296/300\n",
            "106/106 [==============================] - 38s 356ms/step - loss: 0.0928 - auc: 0.9964 - val_loss: 0.2468 - val_auc: 0.9839\n",
            "Epoch 297/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0922 - auc: 0.9966 - val_loss: 0.3976 - val_auc: 0.9803\n",
            "Epoch 298/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0928 - auc: 0.9967 - val_loss: 0.1821 - val_auc: 0.9889\n",
            "Epoch 299/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0919 - auc: 0.9967 - val_loss: 0.1312 - val_auc: 0.9932\n",
            "Epoch 300/300\n",
            "106/106 [==============================] - 38s 357ms/step - loss: 0.0921 - auc: 0.9967 - val_loss: 0.1659 - val_auc: 0.9908\n",
            "Best validation AUC is 0.9932315945625305 on epoch #299\n"
          ]
        }
      ],
      "source": [
        "model = make_model(X_TRAIN.shape[1:])\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "              loss=keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=keras.metrics.AUC(multi_label=True, num_labels=3, name='auc'))\n",
        "\n",
        "history = model.fit(X_TRAIN, Y_TRAIN, validation_split=0.1,\n",
        "                    batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "                    callbacks=[TrackBestPerformance()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-RZ1m8QgDvrr",
        "outputId": "9763657c-211b-4f5e-d96a-1e64fa667823"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best validation AUC: 0.9932315945625305\n",
            "Corresponding train AUC: 0.9966993927955627\n"
          ]
        }
      ],
      "source": [
        "print('Best validation AUC: {}'.format(max(history.history['val_auc'])))\n",
        "print('Corresponding train AUC: {}'.format(history.history['auc'][np.argmax(history.history['val_auc'])]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HzogbTOYKElW",
        "outputId": "362141a5-f0cf-4615-d149-9c195fb9cde9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1380 - auc: 0.9924\n",
            "AUC on test data: 0.9924206733703613\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on test data\n",
        "val_auc = model.evaluate(X_VAL, Y_VAL)[1]\n",
        "print(f'AUC on test data: {val_auc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Nv1ao-sZy6pm"
      },
      "outputs": [],
      "source": [
        "X_BATCHES_TRAIN = np.array_split(X_TRAIN, 50, axis=0)\n",
        "Y_PRED_TRAIN = np.concatenate([model(x_batch).numpy() for x_batch in X_BATCHES_TRAIN], axis=0)\n",
        "\n",
        "X_BATCHES_VAL = np.array_split(X_VAL, 50, axis=0)\n",
        "Y_PRED_VAL = np.concatenate([model(x_batch).numpy() for x_batch in X_BATCHES_VAL], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eNHZ3CRy0V4X"
      },
      "outputs": [],
      "source": [
        "def compute_roc_points(y, y_pred, num_thresholds=200):\n",
        "  thresholds = np.linspace(0., 1., num_thresholds)\n",
        "\n",
        "  tp, fp, tn, fn = [], [], [], []\n",
        "\n",
        "  for threshold in thresholds:\n",
        "    y_pred_label = (y_pred > threshold).astype(int)\n",
        "    tp.append(np.count_nonzero(((y_pred_label == 1) & (y == 1)), axis=0))\n",
        "    fp.append(np.count_nonzero(((y_pred_label == 1) & (y == 0)), axis=0))\n",
        "    tn.append(np.count_nonzero(((y_pred_label == 0) & (y == 0)), axis=0))\n",
        "    fn.append(np.count_nonzero(((y_pred_label == 0) & (y == 1)), axis=0))\n",
        "\n",
        "  tp, fp, tn, fn = np.array(tp), np.array(fp), np.array(tn), np.array(fn)\n",
        "\n",
        "  tp_rate = tp / (tp + fn)\n",
        "  fp_rate = fp / (fp + tn)\n",
        "\n",
        "  return fp_rate, tp_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "n93z-Nnock1c",
        "outputId": "5172c148-7007-49bf-e680-fbb9d2c51fb2"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, axes = plot.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
        "\n",
        "fp_rate_train, tp_rate_train = compute_roc_points(Y_TRAIN, Y_PRED_TRAIN)\n",
        "_ = axes[0].plot(fp_rate_train[:, 0], tp_rate_train[:, 0], label='No')\n",
        "_ = axes[0].plot(fp_rate_train[:, 1], tp_rate_train[:, 1], label='Sphere')\n",
        "_ = axes[0].plot(fp_rate_train[:, 2], tp_rate_train[:, 2], label='Vort')\n",
        "_ = axes[0].set(title='ROC curve for training data')\n",
        "\n",
        "fp_rate_val, tp_rate_val = compute_roc_points(Y_VAL, Y_PRED_VAL)\n",
        "_ = axes[1].plot(fp_rate_val[:, 0], tp_rate_val[:, 0], label='No')\n",
        "_ = axes[1].plot(fp_rate_val[:, 1], tp_rate_val[:, 1], label='Sphere')\n",
        "_ = axes[1].plot(fp_rate_val[:, 2], tp_rate_val[:, 2], label='Vort')\n",
        "_ = axes[1].set(title='ROC curve for test data')\n",
        "\n",
        "_ = plot.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qxVhHyDTxoXZ",
        "outputId": "39908c91-dd5c-4e77-af84-2b95fb171eb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/weights/assets\n"
          ]
        }
      ],
      "source": [
        "# Save model weights\n",
        "model.save(MODELPATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fnbULCuKlhZ"
      },
      "outputs": [],
      "source": [
        "# Compress model weights\n",
        "!zip -r './test_1_weights.zip' './weights'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "my15Kp-aKooo"
      },
      "outputs": [],
      "source": [
        "# Load model weights\n",
        "model = keras.models.load_model(MODELPATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1gq_UhvK3mZ"
      },
      "outputs": [],
      "source": [
        "# Run this cell to evaluate on test data\n",
        "X_TEST, Y_TEST = None, None # load test data here. Shape: (None, 150, 150, 1)\n",
        "\n",
        "# Standardize test data\n",
        "X_TEST = ((X_TEST - np.mean(X_TEST, axis=(1, 2), keepdims=True))\n",
        "          / np.std(X_TEST, axis=(1, 2), keepdims=True))\n",
        "\n",
        "model.evaluate(X_TEST, Y_TEST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCjk9Z0qLo2M"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "gsoc_deeplense_test_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
